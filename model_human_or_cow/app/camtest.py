import argparse
import os
import os.path as osp
import time
import cv2
import torch
import numpy as np
import sys
from datetime import datetime
from ultralytics import YOLO
from loguru import logger

try:
    from yolox.data.data_augment import preproc
    from yolox.exp import get_exp
    from yolox.utils import fuse_model, get_model_info, postprocess
    from yolox.utils.visualize import plot_tracking
    from yolox.tracker.byte_tracker import BYTETracker
    from yolox.tracking_utils.timer import Timer
    
    BYTETRACK_AVAILABLE = True
    logger.info("‚úÖ YOLOX –º–æ–¥—É–ª–∏ –∑–∞–≥—Ä—É–∂–µ–Ω—ã —É—Å–ø–µ—à–Ω–æ")
except ImportError as e:
    BYTETRACK_AVAILABLE = False
    logger.warning(f"‚ö†Ô∏è YOLOX –Ω–µ–¥–æ—Å—Ç—É–ø–µ–Ω: {e}")
    logger.warning("–†–∞–±–æ—Ç–∞–µ—Ç —Ç–æ–ª—å–∫–æ –ø—Ä–æ—Å—Ç–æ–π —Ä–µ–∂–∏–º –∫–∞–º–µ—Ä—ã")


class Predictor(object):
    """–ö–ª–∞—Å—Å –¥–ª—è –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–π"""
    def __init__(self, model, exp, device=torch.device("cpu"), fp16=False):
        self.model = model
        self.num_classes = exp.num_classes
        self.confthre = exp.test_conf
        self.nmsthre = exp.nmsthre
        self.test_size = exp.test_size
        self.device = device
        self.fp16 = fp16
        self.rgb_means = (0.485, 0.456, 0.406)
        self.std = (0.229, 0.224, 0.225)

    def inference(self, img, timer):
        img_info = {"id": 0}
        if isinstance(img, str):
            img_info["file_name"] = osp.basename(img)
            img = cv2.imread(img)
        else:
            img_info["file_name"] = None

        height, width = img.shape[:2]
        img_info["height"] = height
        img_info["width"] = width
        img_info["raw_img"] = img

        img, ratio = preproc(img, self.test_size, self.rgb_means, self.std)
        img_info["ratio"] = ratio
        img = torch.from_numpy(img).unsqueeze(0).float().to(self.device)
        if self.fp16:
            img = img.half()

        with torch.no_grad():
            timer.tic()
            outputs = self.model(img)
            outputs = postprocess(outputs, self.num_classes, self.confthre, self.nmsthre)
        
        return outputs, img_info


class IntegratedCamera:
    def __init__(self, args):
        self.args = args
        self.cap = None
        self.predictor = None
        self.tracker = None
        self.timer = None
        self.frame_count = 0
        self.start_time = time.time()
        
        # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –∫–∞–º–µ—Ä—ã
        self.init_camera()
        
        # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è AI (–µ—Å–ª–∏ –¥–æ—Å—Ç—É–ø–Ω–æ –∏ —Ç—Ä–µ–±—É–µ—Ç—Å—è)
        if BYTETRACK_AVAILABLE and args.mode in ['track', 'both']:
            self.init_ai_tracking()
        elif args.mode in ['track', 'both'] and not BYTETRACK_AVAILABLE:
            logger.error("‚ùå AI —Ä–µ–∂–∏–º –Ω–µ–¥–æ—Å—Ç—É–ø–µ–Ω - YOLOX –Ω–µ —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω")
            logger.info("–ü–µ—Ä–µ–∫–ª—é—á–µ–Ω–∏–µ –Ω–∞ –ø—Ä–æ—Å—Ç–æ–π —Ä–µ–∂–∏–º –∫–∞–º–µ—Ä—ã...")
            self.args.mode = 'simple'
    
    def init_camera(self):
        """–ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –∫–∞–º–µ—Ä—ã"""
        logger.info(f"üé• –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –∫–∞–º–µ—Ä—ã —Å ID: {self.args.camera_id}")
        self.cap = cv2.VideoCapture(self.args.camera_id)
        
        if not self.cap.isOpened():
            raise RuntimeError(f"‚ùå –ù–µ —É–¥–∞–ª–æ—Å—å –æ—Ç–∫—Ä—ã—Ç—å –∫–∞–º–µ—Ä—É —Å ID {self.args.camera_id}")
        
        # –ù–∞—Å—Ç—Ä–æ–π–∫–∞ —Ä–∞–∑—Ä–µ—à–µ–Ω–∏—è (–µ—Å–ª–∏ —É–∫–∞–∑–∞–Ω–æ)
        if self.args.width and self.args.height:
            self.cap.set(cv2.CAP_PROP_FRAME_WIDTH, self.args.width)
            self.cap.set(cv2.CAP_PROP_FRAME_HEIGHT, self.args.height)
        
        # –ü–æ–ª—É—á–µ–Ω–∏–µ –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤ –∫–∞–º–µ—Ä—ã
        self.width = int(self.cap.get(cv2.CAP_PROP_FRAME_WIDTH))
        self.height = int(self.cap.get(cv2.CAP_PROP_FRAME_HEIGHT))
        self.fps = int(self.cap.get(cv2.CAP_PROP_FPS)) or 30
        
        logger.info(f"‚úÖ –ö–∞–º–µ—Ä–∞: {self.width}x{self.height} @ {self.fps}fps")
        
        # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –∑–∞–ø–∏—Å–∏ –≤–∏–¥–µ–æ (–µ—Å–ª–∏ –Ω—É–∂–Ω–æ)
        if self.args.save_video:
            self.init_video_writer()
    
    def init_video_writer(self):
        """–ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –∑–∞–ø–∏—Å–∏ –≤–∏–¥–µ–æ"""
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        
        if self.args.mode == 'simple':
            filename = f"simple_camera_{timestamp}.mp4"
        elif self.args.mode == 'track':
            filename = f"tracking_{timestamp}.mp4"
        else:
            filename = f"integrated_{timestamp}.mp4"
        
        self.output_path = osp.join(self.args.output_dir, filename)
        os.makedirs(self.args.output_dir, exist_ok=True)
        
        fourcc = cv2.VideoWriter_fourcc(*"mp4v")
        self.video_writer = cv2.VideoWriter(
            self.output_path, fourcc, self.fps, (self.width, self.height)
        )
        logger.info(f"üìπ –í–∏–¥–µ–æ –±—É–¥–µ—Ç —Å–æ—Ö—Ä–∞–Ω–µ–Ω–æ: {self.output_path}")
    
    def init_ai_tracking(self):
        """–ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è AI —Ç—Ä–µ–∫–∏–Ω–≥–∞"""
        try:
            logger.info("ü§ñ –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è AI —Ç—Ä–µ–∫–∏–Ω–≥–∞...")
            
            # –ü—Ä–æ–≤–µ—Ä–∫–∞ —Ñ–∞–π–ª–æ–≤ –º–æ–¥–µ–ª–∏
            if not self.args.exp_file:
                # –ü–æ–ø—Ä–æ–±—É–µ–º –Ω–∞–π—Ç–∏ —Å—Ç–∞–Ω–¥–∞—Ä—Ç–Ω—ã–µ —Ñ–∞–π–ª—ã
                possible_exp_files = [
                    "yolox/exp/yolox_s.py",
                    "exps/example/yolox_voc/yolox_voc_s.py",
                    "exps/default/yolox_s.py"
                ]
                for exp_file in possible_exp_files:
                    if os.path.exists(exp_file):
                        self.args.exp_file = exp_file
                        break
                else:
                    logger.warning("‚ö†Ô∏è –§–∞–π–ª —ç–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç–∞ –Ω–µ –Ω–∞–π–¥–µ–Ω, –∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è —Å—Ç–∞–Ω–¥–∞—Ä—Ç–Ω–∞—è –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è")
                    self.args.name = "yolox-s"
            
            # –ó–∞–≥—Ä—É–∑–∫–∞ —ç–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç–∞
            if self.args.exp_file:
                exp = get_exp(self.args.exp_file, self.args.name)
            else:
                # –°–æ–∑–¥–∞–µ–º –º–∏–Ω–∏–º–∞–ª—å–Ω—É—é –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—é
                from yolox.exp.yolox_base import Exp
                exp = Exp()
                exp.num_classes = 80  # COCO classes
                exp.test_conf = self.args.conf
                exp.nmsthre = self.args.nms
                exp.test_size = (self.args.tsize, self.args.tsize)
            
            # –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤
            if self.args.conf is not None:
                exp.test_conf = self.args.conf
            if self.args.nms is not None:
                exp.nmsthre = self.args.nms
            if self.args.tsize is not None:
                exp.test_size = (self.args.tsize, self.args.tsize)
            
            # –£—Å—Ç—Ä–æ–π—Å—Ç–≤–æ
            self.device = torch.device("cuda" if self.args.device == "gpu" and torch.cuda.is_available() else "cpu")
            logger.info(f"üîß –ò—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è —É—Å—Ç—Ä–æ–π—Å—Ç–≤–æ: {self.device}")
            
            # –ú–æ–¥–µ–ª—å
            model = exp.get_model().to(self.device)
            model.eval()
            
            # –ó–∞–≥—Ä—É–∑–∫–∞ —á–µ–∫–ø–æ–∏–Ω—Ç–∞ (–µ—Å–ª–∏ —É–∫–∞–∑–∞–Ω)
            if self.args.ckpt and os.path.exists(self.args.ckpt):
                ckpt = torch.load(self.args.ckpt, map_location="cpu")
                model.load_state_dict(ckpt["model"])
                logger.info(f"‚úÖ –ú–æ–¥–µ–ª—å –∑–∞–≥—Ä—É–∂–µ–Ω–∞ –∏–∑: {self.args.ckpt}")
            else:
                logger.warning("‚ö†Ô∏è –ß–µ–∫–ø–æ–∏–Ω—Ç –Ω–µ –Ω–∞–π–¥–µ–Ω, –∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è –Ω–µ–∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω–∞—è –º–æ–¥–µ–ª—å")
            
            # –û–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è –º–æ–¥–µ–ª–∏
            if self.args.fuse:
                model = fuse_model(model)
            if self.args.fp16:
                model = model.half()
            
            # –°–æ–∑–¥–∞–Ω–∏–µ –ø—Ä–µ–¥–∏–∫—Ç–æ—Ä–∞
            self.predictor = Predictor(
                model, exp, device=self.device, fp16=self.args.fp16
            )
            
            # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è —Ç—Ä–µ–∫–µ—Ä–∞
            self.tracker = BYTETracker(self.args, frame_rate=self.fps)
            self.timer = Timer()
            
            logger.info("‚úÖ AI —Ç—Ä–µ–∫–∏–Ω–≥ –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω —É—Å–ø–µ—à–Ω–æ!")
            
        except Exception as e:
            logger.error(f"‚ùå –û—à–∏–±–∫–∞ –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏–∏ AI: {e}")
            import traceback
            traceback.print_exc()
            self.predictor = None
            self.tracker = None
    
    def run_simple_camera(self):
        """–ü—Ä–æ—Å—Ç–æ–π —Ä–µ–∂–∏–º –∫–∞–º–µ—Ä—ã"""
        logger.info("üé• –ó–∞–ø—É—Å–∫ –ø—Ä–æ—Å—Ç–æ–≥–æ —Ä–µ–∂–∏–º–∞ –∫–∞–º–µ—Ä—ã")
        logger.info("–£–ø—Ä–∞–≤–ª–µ–Ω–∏–µ: ESC/Q - –≤—ã—Ö–æ–¥, S - —Å–∫—Ä–∏–Ω—à–æ—Ç, R - –∑–∞–ø–∏—Å—å")
        
        recording = False
        screenshot_count = 0
        
        while True:
            ret, frame = self.cap.read()
            if not ret:
                logger.error("‚ùå –ù–µ —É–¥–∞–ª–æ—Å—å –ø–æ–ª—É—á–∏—Ç—å –∫–∞–¥—Ä —Å –∫–∞–º–µ—Ä—ã")
                break
            
            self.frame_count += 1
            
            # –î–æ–±–∞–≤–ª–µ–Ω–∏–µ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏ –Ω–∞ –∫–∞–¥—Ä
            current_fps = self.frame_count / (time.time() - self.start_time)
            info_text = f"FPS: {current_fps:.1f} | Frame: {self.frame_count}"
            
            if recording:
                info_text += " | ‚è∫ REC"
                cv2.circle(frame, (30, 30), 10, (0, 0, 255), -1)  # –ö—Ä–∞—Å–Ω–∞—è —Ç–æ—á–∫–∞
            
            cv2.putText(frame, info_text, (10, self.height - 20), 
                       cv2.FONT_HERSHEY_SIMPLEX, 0.6, (0, 255, 0), 2)
            
            # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –∫–∞–¥—Ä–∞ (–µ—Å–ª–∏ –∑–∞–ø–∏—Å—å –≤–∫–ª—é—á–µ–Ω–∞)
            if recording and hasattr(self, 'video_writer'):
                self.video_writer.write(frame)
            
            # –û—Ç–æ–±—Ä–∞–∂–µ–Ω–∏–µ
            cv2.imshow("Integrated Camera - Simple Mode", frame)
            
            # –û–±—Ä–∞–±–æ—Ç–∫–∞ –∫–ª–∞–≤–∏—à
            key = cv2.waitKey(1) & 0xFF
            if key in [27, ord('q'), ord('Q')]:  # ESC –∏–ª–∏ Q
                break
            elif key in [ord('s'), ord('S')]:  # –°–∫—Ä–∏–Ω—à–æ—Ç
                screenshot_count += 1
                timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
                screenshot_path = f"screenshot_{timestamp}_{screenshot_count:03d}.jpg"
                cv2.imwrite(screenshot_path, frame)
                logger.info(f"üì∑ –°–∫—Ä–∏–Ω—à–æ—Ç —Å–æ—Ö—Ä–∞–Ω–µ–Ω: {screenshot_path}")
            elif key in [ord('r'), ord('R')]:  # –ü–µ—Ä–µ–∫–ª—é—á–µ–Ω–∏–µ –∑–∞–ø–∏—Å–∏
                if not hasattr(self, 'video_writer'):
                    self.init_video_writer()
                recording = not recording
                logger.info(f"üìπ –ó–∞–ø–∏—Å—å {'–≤–∫–ª—é—á–µ–Ω–∞' if recording else '–≤—ã–∫–ª—é—á–µ–Ω–∞'}")
    
    def run_ai_tracking(self):
        """–†–µ–∂–∏–º AI —Ç—Ä–µ–∫–∏–Ω–≥–∞"""
        if not self.predictor or not self.tracker:
            logger.error("‚ùå AI —Ç—Ä–µ–∫–∏–Ω–≥ –Ω–µ–¥–æ—Å—Ç—É–ø–µ–Ω")
            logger.info("üîÑ –ü–µ—Ä–µ–∫–ª—é—á–µ–Ω–∏–µ –Ω–∞ –ø—Ä–æ—Å—Ç–æ–π —Ä–µ–∂–∏–º...")
            self.run_simple_camera()
            return
        
        logger.info("ü§ñ –ó–∞–ø—É—Å–∫ AI —Ç—Ä–µ–∫–∏–Ω–≥–∞")
        logger.info("–£–ø—Ä–∞–≤–ª–µ–Ω–∏–µ: ESC/Q - –≤—ã—Ö–æ–¥")
        
        results = []
        
        while True:
            ret, frame = self.cap.read()
            if not ret:
                logger.error("‚ùå –ù–µ —É–¥–∞–ª–æ—Å—å –ø–æ–ª—É—á–∏—Ç—å –∫–∞–¥—Ä —Å –∫–∞–º–µ—Ä—ã")
                break
            
            self.frame_count += 1
            
            try:
                # AI –æ–±—Ä–∞–±–æ—Ç–∫–∞
                outputs, img_info = self.predictor.inference(frame, self.timer)
                
                if outputs[0] is not None:
                    # –¢—Ä–µ–∫–∏–Ω–≥
                    online_targets = self.tracker.update(
                        outputs[0], 
                        [img_info['height'], img_info['width']], 
                        self.predictor.test_size
                    )
                    
                    # –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –æ—Ç–æ–±—Ä–∞–∂–µ–Ω–∏—è
                    online_tlwhs = []
                    online_ids = []
                    online_scores = []
                    
                    for t in online_targets:
                        tlwh = t.tlwh
                        tid = t.track_id
                        vertical = tlwh[2] / tlwh[3] > self.args.aspect_ratio_thresh
                        
                        if tlwh[2] * tlwh[3] > self.args.min_box_area and not vertical:
                            online_tlwhs.append(tlwh)
                            online_ids.append(tid)
                            online_scores.append(t.score)
                            
                            # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
                            results.append(
                                f"{self.frame_count},{tid},{tlwh[0]:.2f},{tlwh[1]:.2f},"
                                f"{tlwh[2]:.2f},{tlwh[3]:.2f},{t.score:.2f},-1,-1,-1\n"
                            )
                    
                    self.timer.toc()
                    
                    # –û—Ç—Ä–∏—Å–æ–≤–∫–∞ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
                    online_im = plot_tracking(
                        img_info['raw_img'], online_tlwhs, online_ids, 
                        frame_id=self.frame_count, 
                        fps=1. / max(1e-5, self.timer.average_time)
                    )
                else:
                    self.timer.toc()
                    online_im = frame
                    
            except Exception as e:
                logger.error(f"‚ùå –û—à–∏–±–∫–∞ AI –æ–±—Ä–∞–±–æ—Ç–∫–∏: {e}")
                online_im = frame
                self.timer.toc() if hasattr(self, 'timer') else None
            
            # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –≤–∏–¥–µ–æ
            if self.args.save_video and hasattr(self, 'video_writer'):
                self.video_writer.write(online_im)
            
            # –û—Ç–æ–±—Ä–∞–∂–µ–Ω–∏–µ
            cv2.imshow("Integrated Camera - AI Tracking", online_im)
            
            # –õ–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ –ø—Ä–æ–≥—Ä–µ—Å—Å–∞
            if self.frame_count % 30 == 0:
                if hasattr(self, 'timer') and self.timer.average_time > 0:
                    fps = 1. / max(1e-5, self.timer.average_time)
                else:
                    fps = self.frame_count / (time.time() - self.start_time)
                logger.info(f'Frame {self.frame_count} | FPS: {fps:.1f}')
            
            # –í—ã—Ö–æ–¥
            if cv2.waitKey(1) & 0xFF in [27, ord('q'), ord('Q')]:
                break
        
        # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ —Ç—Ä–µ–∫–∏–Ω–≥–∞
        if results and self.args.save_results:
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            os.makedirs(self.args.output_dir, exist_ok=True)
            results_path = osp.join(self.args.output_dir, f"tracking_results_{timestamp}.txt")
            with open(results_path, 'w') as f:
                f.writelines(results)
            logger.info(f"üíæ –†–µ–∑—É–ª—å—Ç–∞—Ç—ã —Ç—Ä–µ–∫–∏–Ω–≥–∞ —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã: {results_path}")
    
    def run_combined_mode(self):
        """–ö–æ–º–±–∏–Ω–∏—Ä–æ–≤–∞–Ω–Ω—ã–π —Ä–µ–∂–∏–º"""
        logger.info("üîÑ –ó–∞–ø—É—Å–∫ –∫–æ–º–±–∏–Ω–∏—Ä–æ–≤–∞–Ω–Ω–æ–≥–æ —Ä–µ–∂–∏–º–∞")
        logger.info("–£–ø—Ä–∞–≤–ª–µ–Ω–∏–µ: ESC/Q - –≤—ã—Ö–æ–¥, T - –ø–µ—Ä–µ–∫–ª—é—á–µ–Ω–∏–µ AI, S - —Å–∫—Ä–∏–Ω—à–æ—Ç")
        
        ai_enabled = self.predictor is not None
        ai_active = ai_enabled
        screenshot_count = 0
        
        if not ai_enabled:
            logger.warning("‚ö†Ô∏è AI –Ω–µ–¥–æ—Å—Ç—É–ø–µ–Ω, —Ä–∞–±–æ—Ç–∞–µ—Ç –ø—Ä–æ—Å—Ç–æ–π —Ä–µ–∂–∏–º")
        
        while True:
            ret, frame = self.cap.read()
            if not ret:
                logger.error("‚ùå –ù–µ —É–¥–∞–ª–æ—Å—å –ø–æ–ª—É—á–∏—Ç—å –∫–∞–¥—Ä —Å –∫–∞–º–µ—Ä—ã")
                break
            
            self.frame_count += 1
            original_frame = frame.copy()
            
            # AI –æ–±—Ä–∞–±–æ—Ç–∫–∞ (–µ—Å–ª–∏ –≤–∫–ª—é—á–µ–Ω–∞)
            if ai_active and self.predictor and self.tracker:
                try:
                    outputs, img_info = self.predictor.inference(frame, self.timer)
                    
                    if outputs[0] is not None:
                        online_targets = self.tracker.update(
                            outputs[0], 
                            [img_info['height'], img_info['width']], 
                            self.predictor.test_size
                        )
                        
                        online_tlwhs = []
                        online_ids = []
                        online_scores = []
                        
                        for t in online_targets:
                            tlwh = t.tlwh
                            tid = t.track_id
                            vertical = tlwh[2] / tlwh[3] > self.args.aspect_ratio_thresh
                            
                            if tlwh[2] * tlwh[3] > self.args.min_box_area and not vertical:
                                online_tlwhs.append(tlwh)
                                online_ids.append(tid)
                                online_scores.append(t.score)
                        
                        self.timer.toc()
                        frame = plot_tracking(
                            img_info['raw_img'], online_tlwhs, online_ids, 
                            frame_id=self.frame_count, 
                            fps=1. / max(1e-5, self.timer.average_time)
                        )
                    else:
                        self.timer.toc()
                except Exception as e:
                    logger.error(f"‚ùå –û—à–∏–±–∫–∞ AI –æ–±—Ä–∞–±–æ—Ç–∫–∏: {e}")
                    frame = original_frame
            
            # –î–æ–±–∞–≤–ª–µ–Ω–∏–µ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏ –Ω–∞ –∫–∞–¥—Ä
            current_fps = self.frame_count / (time.time() - self.start_time)
            status = "AI ON" if ai_active else "AI OFF"
            info_text = f"FPS: {current_fps:.1f} | {status} | Frame: {self.frame_count}"
            
            cv2.putText(frame, info_text, (10, 30), 
                       cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 255, 0), 2)
            
            # –ò–Ω–¥–∏–∫–∞—Ç–æ—Ä —Ä–µ–∂–∏–º–∞
            color = (0, 255, 0) if ai_active else (0, 0, 255)
            cv2.circle(frame, (self.width - 30, 30), 10, color, -1)
            
            # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –≤–∏–¥–µ–æ
            if self.args.save_video and hasattr(self, 'video_writer'):
                self.video_writer.write(frame)
            
            # –û—Ç–æ–±—Ä–∞–∂–µ–Ω–∏–µ
            cv2.imshow("Integrated Camera - Combined Mode", frame)
            
            # –û–±—Ä–∞–±–æ—Ç–∫–∞ –∫–ª–∞–≤–∏—à
            key = cv2.waitKey(1) & 0xFF
            if key in [27, ord('q'), ord('Q')]:  # –í—ã—Ö–æ–¥
                break
            elif key in [ord('t'), ord('T')] and ai_enabled:  # –ü–µ—Ä–µ–∫–ª—é—á–µ–Ω–∏–µ AI
                ai_active = not ai_active
                logger.info(f"üîÑ AI —Ç—Ä–µ–∫–∏–Ω–≥ {'–≤–∫–ª—é—á–µ–Ω' if ai_active else '–≤—ã–∫–ª—é—á–µ–Ω'}")
            elif key in [ord('s'), ord('S')]:  # –°–∫—Ä–∏–Ω—à–æ—Ç
                screenshot_count += 1
                timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
                screenshot_path = f"screenshot_{timestamp}_{screenshot_count:03d}.jpg"
                cv2.imwrite(screenshot_path, frame)
                logger.info(f"üì∑ –°–∫—Ä–∏–Ω—à–æ—Ç —Å–æ—Ö—Ä–∞–Ω–µ–Ω: {screenshot_path}")
    
    def run(self):
        """–ì–ª–∞–≤–Ω—ã–π –º–µ—Ç–æ–¥ –∑–∞–ø—É—Å–∫–∞"""
        try:
            if self.args.mode == 'simple':
                self.run_simple_camera()
            elif self.args.mode == 'track':
                self.run_ai_tracking()
            elif self.args.mode == 'both':
                self.run_combined_mode()
            else:
                logger.error(f"‚ùå –ù–µ–∏–∑–≤–µ—Å—Ç–Ω—ã–π —Ä–µ–∂–∏–º: {self.args.mode}")
        
        except KeyboardInterrupt:
            logger.info("üëã –û—Å—Ç–∞–Ω–æ–≤–∫–∞ –ø–æ Ctrl+C")
        except Exception as e:
            logger.error(f"‚ùå –û—à–∏–±–∫–∞ –≤—ã–ø–æ–ª–Ω–µ–Ω–∏—è: {e}")
            import traceback
            traceback.print_exc()
        finally:
            self.cleanup()
    
    def cleanup(self):
        """–û—á–∏—Å—Ç–∫–∞ —Ä–µ—Å—É—Ä—Å–æ–≤"""
        if self.cap:
            self.cap.release()
        if hasattr(self, 'video_writer'):
            self.video_writer.release()
        cv2.destroyAllWindows()
        logger.info("üßπ –†–µ—Å—É—Ä—Å—ã –æ—Å–≤–æ–±–æ–∂–¥–µ–Ω—ã")


def make_parser():
    parser = argparse.ArgumentParser(
        "Integrated Camera Application",
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
–ü—Ä–∏–º–µ—Ä—ã –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è:
  python integrated_camera.py simple                    # –ü—Ä–æ—Å—Ç–∞—è –∫–∞–º–µ—Ä–∞
  python integrated_camera.py track                     # AI —Ç—Ä–µ–∫–∏–Ω–≥
  python integrated_camera.py both                      # –ö–æ–º–±–∏–Ω–∏—Ä–æ–≤–∞–Ω–Ω—ã–π —Ä–µ–∂–∏–º
  python integrated_camera.py simple --save_video       # –° —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ–º –≤–∏–¥–µ–æ
        """
    )
    
    # –û—Å–Ω–æ–≤–Ω—ã–µ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã
    parser.add_argument("mode", choices=["simple", "track", "both"], 
                       help="–†–µ–∂–∏–º —Ä–∞–±–æ—Ç—ã")
    
    # –ü–∞—Ä–∞–º–µ—Ç—Ä—ã –∫–∞–º–µ—Ä—ã
    parser.add_argument("--camera_id", type=int, default=0, help="ID –∫–∞–º–µ—Ä—ã")
    parser.add_argument("--width", type=int, default=None, help="–®–∏—Ä–∏–Ω–∞ –∫–∞–¥—Ä–∞")
    parser.add_argument("--height", type=int, default=None, help="–í—ã—Å–æ—Ç–∞ –∫–∞–¥—Ä–∞")
    
    # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ
    parser.add_argument("--save_video", action="store_true", help="–°–æ—Ö—Ä–∞–Ω—è—Ç—å –≤–∏–¥–µ–æ")
    parser.add_argument("--save_results", action="store_true", help="–°–æ—Ö—Ä–∞–Ω—è—Ç—å —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã —Ç—Ä–µ–∫–∏–Ω–≥–∞")
    parser.add_argument("--output_dir", type=str, default="./output", help="–ü–∞–ø–∫–∞ –¥–ª—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è")
    
    # AI –ø–∞—Ä–∞–º–µ—Ç—Ä—ã
    parser.add_argument("-f", "--exp_file", type=str, default=None, help="–§–∞–π–ª —ç–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç–∞")
    parser.add_argument("-n", "--name", type=str, default=None, help="–ò–º—è –º–æ–¥–µ–ª–∏")
    parser.add_argument("-c", "--ckpt", type=str, default=None, help="–ß–µ–∫–ø–æ–∏–Ω—Ç –º–æ–¥–µ–ª–∏")
    parser.add_argument("--device", type=str, default="cpu", help="–£—Å—Ç—Ä–æ–π—Å—Ç–≤–æ: cpu –∏–ª–∏ gpu")
    parser.add_argument("--conf", type=float, default=0.5, help="–ü–æ—Ä–æ–≥ —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç–∏")
    parser.add_argument("--nms", type=float, default=0.45, help="–ü–æ—Ä–æ–≥ NMS")
    parser.add_argument("--tsize", type=int, default=640, help="–†–∞–∑–º–µ—Ä –≤—Ö–æ–¥–Ω–æ–≥–æ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è")
    parser.add_argument("--fp16", action="store_true", help="–ò—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å FP16")
    parser.add_argument("--fuse", action="store_true", help="–°–ª–∏—Ç—å conv –∏ bn")
    
    # –ü–∞—Ä–∞–º–µ—Ç—Ä—ã —Ç—Ä–µ–∫–∏–Ω–≥–∞
    parser.add_argument("--track_thresh", type=float, default=0.5, help="–ü–æ—Ä–æ–≥ —Ç—Ä–µ–∫–∏–Ω–≥–∞")
    parser.add_argument("--track_buffer", type=int, default=30, help="–ë—É—Ñ–µ—Ä —Ç—Ä–µ–∫–∏–Ω–≥–∞")
    parser.add_argument("--match_thresh", type=float, default=0.8, help="–ü–æ—Ä–æ–≥ —Å–æ–ø–æ—Å—Ç–∞–≤–ª–µ–Ω–∏—è")
    parser.add_argument("--aspect_ratio_thresh", type=float, default=1.6, help="–ü–æ—Ä–æ–≥ —Å–æ–æ—Ç–Ω–æ—à–µ–Ω–∏—è —Å—Ç–æ—Ä–æ–Ω")
    parser.add_argument("--min_box_area", type=float, default=10, help="–ú–∏–Ω–∏–º–∞–ª—å–Ω–∞—è –ø–ª–æ—â–∞–¥—å –±–æ–∫—Å–∞")
    parser.add_argument("--mot20", action="store_true", help="–†–µ–∂–∏–º MOT20")
    
    return parser


if __name__ == "__main__":
    # –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏—è
    logger.remove()
    logger.add(
        lambda msg: print(msg, end=""), 
        format="<green>{time:HH:mm:ss}</green> | <level>{level}</level> | {message}"
    )
    
    # –ü—Ä–æ–≤–µ—Ä–∫–∞ –∞—Ä–≥—É–º–µ–Ω—Ç–æ–≤
    if len(sys.argv) == 1:
        parser = make_parser()
        parser.print_help()
        print("\nüöÄ –ë—ã—Å—Ç—Ä—ã–π —Å—Ç–∞—Ä—Ç:")
        print("python integrated_camera.py simple      # –ü—Ä–æ—Å—Ç–∞—è –∫–∞–º–µ—Ä–∞")
        print("python integrated_camera.py track       # AI —Ç—Ä–µ–∫–∏–Ω–≥")
        print("python integrated_camera.py both        # –ö–æ–º–±–∏–Ω–∏—Ä–æ–≤–∞–Ω–Ω—ã–π")
        sys.exit(0)
    
    # –ü–∞—Ä—Å–∏–Ω–≥ –∞—Ä–≥—É–º–µ–Ω—Ç–æ–≤
    parser = make_parser()
    args = parser.parse_args()
    
    logger.info("üöÄ –ó–∞–ø—É—Å–∫ –∏–Ω—Ç–µ–≥—Ä–∏—Ä–æ–≤–∞–Ω–Ω–æ–≥–æ –ø—Ä–∏–ª–æ–∂–µ–Ω–∏—è –∫–∞–º–µ—Ä—ã")
    logger.info(f"üìã –†–µ–∂–∏–º: {args.mode}")
    
    # –°–æ–∑–¥–∞–Ω–∏–µ –∏ –∑–∞–ø—É—Å–∫ –ø—Ä–∏–ª–æ–∂–µ–Ω–∏—è
    app = IntegratedCamera(args)
    app.run()